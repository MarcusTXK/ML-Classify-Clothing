{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4acb7a-64dd-4107-9ee4-d0ee582724f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2 as cv\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imshow, imsave\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6dff234-7a99-4b49-88af-0a40d98ef085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "\n",
    "file = glob.glob('./clothing-dataset-small-master.zip')\n",
    "\n",
    "with zipfile.ZipFile(file[0], 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0f5491-21de-454f-ab7e-d7dcc901a7df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17316\\4035382748.py:21: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n",
      "  img = img.transpose(Image.FLIP_LEFT_RIGHT)\n"
     ]
    }
   ],
   "source": [
    "for data_type in [\"train\", \"validation\", \"test\"]:\n",
    "    folder_path = \"./data/clothing-dataset-small-master/\" + data_type\n",
    "    \n",
    "    # combine validation and train\n",
    "    new_folder_path = \"./data/clothing-dataset/\" + (\"test\" if data_type == \"test\" else \"train\")\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        new_path = os.path.join(new_folder_path, folder)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                new_image_path = os.path.join(new_folder_path, folder, filename)\n",
    "                img.save(new_image_path)\n",
    "                                              \n",
    "                # flip all images besides t-shirts to balance the data\n",
    "                if folder != \"t-shirt\":\n",
    "                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                    new_image_path = os.path.join(new_folder_path, folder, \"flipped_\" + filename)\n",
    "                    img.save(new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577498db-2c22-45a8-975d-95e6806d55ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate SIFT data from above data\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "\n",
    "for data_type in data_types:\n",
    "    folder_path = \"./data/clothing-dataset/\" + data_type\n",
    "    new_folder_path = \"./data/clothing-dataset-sift/\" + data_type\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        new_path = os.path.join(new_folder_path, folder)\n",
    "        \n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            # Open the image\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "                resized_img  = cv.resize(img, (64, 128),interpolation =cv.INTER_LINEAR)\n",
    "                # Apply the SIFT algorithm\n",
    "                sift = cv.SIFT_create()\n",
    "                keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "                # Draw the keypoints on the image\n",
    "                img_with_keypoints = cv.drawKeypoints(img, keypoints, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "                # Save the image with keypoints to the new folder\n",
    "                new_image_path = os.path.join(new_folder_path, folder, filename)\n",
    "                cv.imwrite(new_image_path, img_with_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd88eab1-3b96-46c5-b42c-185e1e67903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: SIFT\n",
    "from imageio import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_histogram(descriptors, kmeans_model):\n",
    "    histogram = np.zeros(len(kmeans_model.cluster_centers_))\n",
    "    cluster_assignments = kmeans_model.predict(descriptors)\n",
    "    for i in cluster_assignments:\n",
    "        histogram[i] += 1\n",
    "    return histogram\n",
    "\n",
    "# Feature: SIFT\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "train_sift_descriptors = []\n",
    "y_train = []\n",
    "test_sift_descriptors = []\n",
    "y_test = []\n",
    "\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "for data_type in data_types:\n",
    "    folder_path = \"./data/clothing-dataset-sift/\" + data_type\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            # Open the image\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "                _, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "                if data_type == \"test\":\n",
    "                    test_sift_descriptors.append(descriptors)\n",
    "                    y_test.append(folder)\n",
    "                else:\n",
    "                    train_sift_descriptors.append(descriptors)\n",
    "                    y_train.append(folder)\n",
    "\n",
    "# Cluster the descriptors using KMeans to create a visual vocabulary\n",
    "kmeans = KMeans(n_clusters=50)\n",
    "all_descriptors = np.vstack(train_sift_descriptors)\n",
    "kmeans.fit(all_descriptors)\n",
    "\n",
    "# Create histograms (BoVW) for each image using the visual vocabulary\n",
    "train_histograms = [create_histogram(desc, kmeans) for desc in train_sift_descriptors]\n",
    "test_histograms = [create_histogram(desc, kmeans) for desc in test_sift_descriptors]\n",
    "\n",
    "# Normalize the histograms\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_histograms)\n",
    "X_test = scaler.transform(test_histograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d608ef-a4fe-4205-91f9-eeec0d87c15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# define the number of folds for cross validation\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# define param search space for knn, dt, and rf\n",
    "knn_param_grid = {'n_neighbors': list(range(2, 100))}\n",
    "dt_param_grid = {'max_depth': list(range(2, 100))}\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': list(range(2, 100)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "svm_param_grid = {'svm__alpha': uniform(loc=0, scale=0.01), \n",
    "                  'svm__loss': ['hinge', 'squared_hinge'], \n",
    "                 'svm__penalty': ['l2', 'l1', 'elasticnet']}\n",
    "\n",
    "# define models\n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier(max_features=512)\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# define scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0446c4-811f-4794-bb3d-4847852e8a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Best k:  {'n_neighbors': 2}\n",
      "Best F1 score:  0.5254101146027134\n",
      "Test F1 Score:  0.13728323699421965\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn_model = RandomizedSearchCV(knn, knn_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, random_state=42)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN:\")\n",
    "print(\"Best k: \", knn_model.best_params_)\n",
    "print(\"Best F1 score: \", knn_model.best_score_)\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=int(knn_model.best_params_['n_neighbors']))\n",
    "knn_final.fit(X_train, y_train)\n",
    "y_pred = knn_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb3ae1cf-7328-4e18-9a80-952d753cbd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT:\n",
      "Best depth:  {'max_depth': 83}\n",
      "Best F1 score:  0.2675886988754945\n",
      "Test F1 Score:  0.16329479768786126\n"
     ]
    }
   ],
   "source": [
    "# DT\n",
    "dt_model = RandomizedSearchCV(dt, dt_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"DT:\")\n",
    "print(\"Best depth: \", dt_model.best_params_)\n",
    "print(\"Best F1 score: \", dt_model.best_score_)\n",
    "\n",
    "dt_final = DecisionTreeClassifier(max_depth=int(dt_model.best_params_['max_depth']), max_features=512)\n",
    "dt_final.fit(X_train, y_train)\n",
    "y_pred = dt_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31240f3-65ad-4173-b565-541e3a2b0d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Best parameters:  {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 74}\n",
      "Best F1 score:  0.43789804407970934\n",
      "Test F1 Score:  0.22254335260115607\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (RF)\n",
    "rf_model = RandomizedSearchCV(rf, rf_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(\"Best parameters: \", rf_model.best_params_)\n",
    "print(\"Best F1 score: \", rf_model.best_score_)\n",
    "\n",
    "rf_final = RandomForestClassifier(**rf_model.best_params_)\n",
    "rf_final.fit(X_train, y_train)\n",
    "y_pred_rf = rf_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred_rf, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e0dc30f-255e-4cc7-b19d-df305fc3f58c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Best params:  {'svm__alpha': 0.007319939418114051, 'svm__loss': 'hinge', 'svm__penalty': 'l2'}\n",
      "Best F1 score:  0.23089558325050336\n",
      "Test F1 Score:  0.19508670520231214\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# SVM\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SGDClassifier())\n",
    "])\n",
    "\n",
    "svm_model = RandomizedSearchCV(pipeline, svm_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM\")\n",
    "print(\"Best params: \", svm_model.best_params_)\n",
    "print(\"Best F1 score: \", svm_model.best_score_)\n",
    "\n",
    "svm_final = make_pipeline(StandardScaler(), SGDClassifier(alpha=svm_model.best_params_['svm__alpha'], loss=svm_model.best_params_['svm__loss'], penalty=svm_model.best_params_['svm__penalty']))\n",
    "svm_final.fit(X_train, y_train)\n",
    "y_pred = svm_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c58f7e-1ed3-4036-8ac3-c2a4603bed26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
